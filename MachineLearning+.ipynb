{"nbformat_minor": 2, "cells": [{"source": "## Create a toy dataframe from dictionary", "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "toy_dct = [\n    {\"val1\": 123,\"val2\":2,'label':1},\n    {\"val1\": 234,\"val2\":54,'label':0},\n    {\"val1\": 354,\"val2\":6,'label':1},\n    {\"val1\": 78,\"val2\":56,'label':1},\n    {\"val1\": 234,\"val2\":12,'label':0},\n    {\"val1\": 942,\"val2\":76,'label':0}\n]", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 33.849853515625, "end_time": 1574045159647.082}}, "collapsed": false}}, {"execution_count": 34, "cell_type": "code", "source": "toy_df = spark.createDataFrame(toy_dct)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 237.569091796875, "end_time": 1574045160392.981}}, "collapsed": false}}, {"execution_count": 35, "cell_type": "code", "source": "toy_df.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+----+----+\n|label|val1|val2|\n+-----+----+----+\n|    1| 123|   2|\n|    0| 234|  54|\n|    1| 354|   6|\n|    1|  78|  56|\n|    0| 234|  12|\n|    0| 942|  76|\n+-----+----+----+"}], "metadata": {"cell_status": {"execute_time": {"duration": 750.364990234375, "end_time": 1574045161857.95}}, "collapsed": false}}, {"source": "## Machine learning", "cell_type": "markdown", "metadata": {}}, {"source": "### prepare features matrix", "cell_type": "markdown", "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": "from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 40.468994140625, "end_time": 1574045163582.337}}, "collapsed": false}}, {"execution_count": 37, "cell_type": "code", "source": "toy_df.printSchema()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- label: long (nullable = true)\n |-- val1: long (nullable = true)\n |-- val2: long (nullable = true)"}], "metadata": {"cell_status": {"execute_time": {"duration": 32.658935546875, "end_time": 1574045163774.983}}, "collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "#create a features column\ncolumns = toy_df.schema.names \nfeatures = [col for col in columns if col != \"label\"]\nfeatures", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['val1', 'val2']"}], "metadata": {"cell_status": {"execute_time": {"duration": 31.379150390625, "end_time": 1574045171251.715}}, "collapsed": false}}, {"execution_count": 40, "cell_type": "code", "source": "vectorAssembler = VectorAssembler(inputCols = features,outputCol='features')", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 241.06787109375, "end_time": 1574045172255.391}}, "collapsed": true}}, {"execution_count": 41, "cell_type": "code", "source": "class_df = vectorAssembler.transform(toy_df)\nclass_df = class_df.select(['features','label'])", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 241.614990234375, "end_time": 1574045175439.355}}, "collapsed": false}}, {"execution_count": 42, "cell_type": "code", "source": "class_df.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+-----+\n|    features|label|\n+------------+-----+\n| [123.0,2.0]|    1|\n|[234.0,54.0]|    0|\n| [354.0,6.0]|    1|\n| [78.0,56.0]|    1|\n|[234.0,12.0]|    0|\n|[942.0,76.0]|    0|\n+------------+-----+"}], "metadata": {"cell_status": {"execute_time": {"duration": 752.52392578125, "end_time": 1574045177124.957}}, "collapsed": false}}, {"source": "### train test split", "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": "splits = class_df.randomSplit([0.66, 0.333])\ntrain_df = splits[0]\ntest_df = splits[1]", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 35.7470703125, "end_time": 1574045179318.834}}, "collapsed": true}}, {"execution_count": 44, "cell_type": "code", "source": "train_df.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+-----+\n|    features|label|\n+------------+-----+\n| [123.0,2.0]|    1|\n| [354.0,6.0]|    1|\n| [78.0,56.0]|    1|\n|[234.0,12.0]|    0|\n+------------+-----+"}], "metadata": {"cell_status": {"execute_time": {"duration": 743.35888671875, "end_time": 1574045182393.685}}, "collapsed": false}}, {"execution_count": 45, "cell_type": "code", "source": "test_df.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+-----+\n|    features|label|\n+------------+-----+\n|[234.0,54.0]|    0|\n|[942.0,76.0]|    0|\n+------------+-----+"}], "metadata": {"cell_status": {"execute_time": {"duration": 759.177978515625, "end_time": 1574045184419.243}}, "collapsed": false}}, {"source": "## Train", "cell_type": "markdown", "metadata": {}}, {"execution_count": 46, "cell_type": "code", "source": "lr = LogisticRegression(maxIter=10)\nmodel = lr.fit(train_df)\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(model.coefficients))\nprint(\"Intercept: \" + str(model.intercept))\n\n", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Coefficients: [-0.00274868939276,0.0165602064037]\nIntercept: 1.4106960521079925"}], "metadata": {"cell_status": {"execute_time": {"duration": 7302.93896484375, "end_time": 1574045194393.975}}, "collapsed": false}}, {"execution_count": 50, "cell_type": "code", "source": "", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.75"}], "metadata": {"cell_status": {"execute_time": {"duration": 744.807861328125, "end_time": 1574045327247.411}}, "collapsed": false}}, {"execution_count": 54, "cell_type": "code", "source": "train_sum = model.summary\naccuracy = train_sum.accuracy\nprint(\"Accuracy {}\".format(accuracy))\ntrain_sum.roc.show()\nprint(\"areaUnderROC: \" + str(train_sum.areaUnderROC))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Accuracy 0.75\n+---+------------------+\n|FPR|               TPR|\n+---+------------------+\n|0.0|               0.0|\n|0.0|0.3333333333333333|\n|0.0|0.6666666666666666|\n|1.0|0.6666666666666666|\n|1.0|               1.0|\n|1.0|               1.0|\n+---+------------------+\n\nareaUnderROC: 0.6666666666666666"}], "metadata": {"cell_status": {"execute_time": {"duration": 1268.06005859375, "end_time": 1574045385348.025}}, "collapsed": false}}, {"execution_count": 58, "cell_type": "code", "source": "train_sum.predictions.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+-----+--------------------+--------------------+----------+\n|    features|label|       rawPrediction|         probability|prediction|\n+------------+-----+--------------------+--------------------+----------+\n| [123.0,2.0]|  1.0|[-1.1057276696057...|[0.24866824068665...|       1.0|\n| [354.0,6.0]|  1.0|[-0.5370212454927...|[0.36888078814599...|       1.0|\n| [78.0,56.0]|  1.0|[-2.1236698380787...|[0.10681743497387...|       1.0|\n|[234.0,12.0]|  0.0|[-0.9662252110461...|[0.27563353673688...|       1.0|\n+------------+-----+--------------------+--------------------+----------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 751.376953125, "end_time": 1574045508911.33}}, "collapsed": false}}, {"source": "## Test prediction", "cell_type": "markdown", "metadata": {}}, {"execution_count": 72, "cell_type": "code", "source": "predictions = model.transform(test_df)\npredictions.select(\"prediction\",\"label\")\nprint(\"Coefficients: \" + str(predictions.coefficients))\nprint(\"Intercept: \" + str(predictions.intercept))", "outputs": [{"output_type": "stream", "name": "stderr", "text": "'DataFrame' object has no attribute 'coefficients'\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py\", line 1300, in __getattr__\n    \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\nAttributeError: 'DataFrame' object has no attribute 'coefficients'\n\n"}], "metadata": {"cell_status": {"execute_time": {"duration": 241.329833984375, "end_time": 1574046072954.776}}, "collapsed": true}}, {"execution_count": 69, "cell_type": "code", "source": "predictions.show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+------------+-----+--------------------+--------------------+----------+\n|    features|label|       rawPrediction|         probability|prediction|\n+------------+-----+--------------------+--------------------+----------+\n|[234.0,54.0]|    0|[-1.6617538800007...|[0.15952669989319...|       1.0|\n|[942.0,76.0]|    0|[-0.0800063308070...|[0.48000907967247...|       1.0|\n+------------+-----+--------------------+--------------------+----------+"}], "metadata": {"cell_status": {"execute_time": {"duration": 744.22412109375, "end_time": 1574046010613.084}}, "collapsed": false}}, {"source": "## Test set evaluation", "cell_type": "markdown", "metadata": {}}, {"execution_count": 73, "cell_type": "code", "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 50.123046875, "end_time": 1574046295275.12}}, "collapsed": true}}, {"execution_count": 77, "cell_type": "code", "source": "test_evaluator = BinaryClassificationEvaluator(rawPredictionCol='rawPrediction')\ntest_evaluator.evaluate(predictions) #AUC", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 237.97607421875, "end_time": 1574046591116.538}}, "collapsed": false}}, {"execution_count": 82, "cell_type": "code", "source": "test_evaluator.evaluate(predictions) #AUC", "outputs": [{"output_type": "stream", "name": "stdout", "text": "0.0"}], "metadata": {"cell_status": {"execute_time": {"duration": 747.2001953125, "end_time": 1574046849351.777}}, "collapsed": false}}, {"execution_count": 81, "cell_type": "code", "source": "test_evaluator.getMetricName()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "'areaUnderROC'"}], "metadata": {"cell_status": {"execute_time": {"duration": 49.875, "end_time": 1574046628990.369}}, "collapsed": false}}, {"execution_count": 90, "cell_type": "code", "source": "import numpy as np\nprediction_for_one =  np.array(predictions.select([\"prediction\"]).collect())[0]", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 246.859130859375, "end_time": 1574047002510.886}}, "collapsed": false}}, {"execution_count": 91, "cell_type": "code", "source": "prediction_for_one", "outputs": [{"output_type": "stream", "name": "stdout", "text": "array([ 1.])"}], "metadata": {"cell_status": {"execute_time": {"duration": 30.721923828125, "end_time": 1574047011887.32}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}